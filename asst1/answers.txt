Program 2

VECTOR_WIDTH | vector utilization
-------------+--------------------
      2      |      92.4%
      4      |      90.8%
      8      |      89.9%
     16      |      89.5% 
Vector utilization decreases as VECTOR_WIDTH changes. In our implementation, we loop through multiplies up to the maximum exponent 
in the current vector. When the vector width is larger, this results in more amount of multiplies even for smaller exponents in the chunk. 
For example, if one chunk of exponents is 100 2 2 2, then a vector chunk of 2 is better than a vector chunk of 4. 

====================================================================================================================================

Program 3

Part 1] For view 1, the speedup we get is 5.06x using ISPC, and for view 2, the speedup we get is 4.33x using ISPC.
The maximum speedup we would expect from these CPUs would be 8x, because they do eight-wide SIMD vector instructions, meaning 
the values for 8 pixels could be computed at once. 
However, we do not get the maximum speedup, most likely due to different number of iterations of the for loop in the mandel() 
function for different elements in each gang. We know that all elements in the 8-wide vector need to execute to completion, which means 
each execution is bottlenecked by the slowest element (before the next gang can be computed). 
We can confirm this with the difference between image 1 and 2 --- there is more of a speedup with view 1, likely because there are more 
chunks of consecutive pixels that have similar values (num. of loops in mandel()), versus in view 2, the densities are more spread out and
less balanced within one vector. 
(The parts of the image that present challenges for SIMD execution would be 8 consecutive pixels that have different end values / number 
of loops in mandel().)

Part 2]
1. The speedup is 9.87x using task ISPC, which was 1.95x speedup over the version without tasks. 
2. We change the number of tasks the code creates from 2 to 16, which gives a 32.18x speedup over the sequential version of the code. 
We chose 16 tasks because **[ TODO ]**. (We found with 16 and 32 tasks, there was a similar speedup.)

TODO * Try non- powers of 2

====================================================================================================================================

Program 4

1. The ISPC implementation speedup for single CPU core (no tasks) is 4.37x. 
The ISPC implementation speedup when using all cores (with tasks) is 31.65x. 

The speedup due to SIMD parallelization is 4.37x. 
The speedup due to multi-core parallelization 7.24x. 

2. The input we choose is if each element in values is 2.99999. 
The resulting speedup is 6.83x from ISPC, and 46.66x from task ISPC. 
The modification improves SIMD speedup. This is because every number will require lots of iterations to converge, but 
every element in the vector would require the same (large) number of iterations, and they would converge at once. 
There is not much of a speedup for multi-core (which is 46.66/6.83 = 6.83 compared to 7.24) because **[ TODO ]**

3. The input we choose is if all the elements in values is 1, except every 8th element is 2.78. (**TODO: which we tuned?**)
The resulting speedup is 0.84x from ISPC, and 5.09x from task ISPC. 
We chose this input if we add a longer iteration number every 8th element, this forces the ISPC vector to take additional iterations
for all the other elements which are not necessary, and in the sequential implementation, those other elements would converge very 
quickly so there is not much of a difference. 

TODO * for q3, go with 2.99999 for now because it's easier to explain

====================================================================================================================================

Program 6

First, we timed the average number of seconds that each of the functions computeAssignments(), computeCentroids(), and computeCost() took in 
iterations of the loop. 

computeAssignments: 0.301481
computeCentroids: 0.0923297
computeCost: 0.0440835

We see that most of the time being spent in the code is in computeAssignments(), which is where we decided to focus our speedup. 


After some modifications: (not enough optimizing!)
assign_comp: 0.0510605
centroids_comp: 0.0389844
cost_comp: 0.0703796
[Total Time]: 5656.665 ms

TODO * Think about the order of the loops & which variable is the largest. Think about data reuse + caches when using k as outer loop and m as inner loop. data eviction; caches.
